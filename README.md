# Desafio TÃ©cnico Veeries: Pipeline de Dados de Lineup PortuÃ¡rio

## ğŸ“ DescriÃ§Ã£o do Projeto
Este projeto implementa um pipeline de dados ETL (ExtraÃ§Ã£o, TransformaÃ§Ã£o e Carga) em Python para coletar, processar e agregar diariamente os dados de lineup de navios dos portos de Santos e ParanaguÃ¡. O objetivo final Ã© gerar uma base de dados analÃ­tica com os volumes diÃ¡rios movimentados por porto, produto e sentido (importaÃ§Ã£o/exportaÃ§Ã£o).

## ğŸ›ï¸ Arquitetura
A soluÃ§Ã£o foi desenvolvida utilizando a arquitetura Medallion, que separa os dados em trÃªs camadas lÃ³gicas:
* **ğŸ¥‰ Camada Bronze:** ContÃ©m os dados brutos, extraÃ­dos diretamente das fontes, sem nenhuma alteraÃ§Ã£o (arquivos HTML). Isso garante a rastreabilidade e a capacidade de reprocessamento.
* **ğŸ¥ˆ Camada Silver:** Armazena os dados apÃ³s um processo de limpeza, padronizaÃ§Ã£o de esquema, conversÃ£o de tipos e unificaÃ§Ã£o das diferentes fontes em um formato Ãºnico e consistente (Parquet).
* **ğŸ¥‡ Camada Gold:** Apresenta os dados prontos para o consumo, agregados por dimensÃµes de negÃ³cio para responder Ã  pergunta central do desafio: volumes diÃ¡rios.

## ğŸš€ Como Executar
1.  **Clone o repositÃ³rio:**
    ```bash
    git clone [https://github.com/danielfloriani/lineup_ships]
    cd [lineup_ships]
    ```
2.  **Crie um ambiente virtual (recomendado):**
    ```bash
    python -m venv venv
    source venv/bin/activate  # No Windows: venv\Scripts\activate
    ```
3.  **Instale as dependÃªncias:**
    ```bash
    pip install -r requirements.txt
    ```
4.  **Execute o pipeline completo:**
    ```bash
    python pipeline.py
    ```
    Ao final da execuÃ§Ã£o, os resultados estarÃ£o na pasta `data/`, com o arquivo final em `data/gold/volumes_diarios.parquet`.

### ğŸ› ï¸ Estrutura do Projeto

O repositÃ³rio estÃ¡ organizado de forma modular para separar as diferentes responsabilidades do pipeline, seguindo as melhores prÃ¡ticas de engenharia de software.

/
â”œâ”€â”€ data/                   # DiretÃ³rio dos dados (ignorado pelo .gitignore)
â”‚   â”œâ”€â”€ bronze/             # Camada Bronze: Armazena os dados brutos e inalterados (HTMLs).
â”‚   â”œâ”€â”€ silver/             # Camada Silver: Armazena os dados limpos e consolidados (Parquet).
â”‚   â””â”€â”€ gold/               # Camada Gold: Armazena os dados agregados e prontos para anÃ¡lise (Parquet).
â”‚
â”œâ”€â”€ src/                    # DiretÃ³rio principal do cÃ³digo-fonte da aplicaÃ§Ã£o.
â”‚   â”œâ”€â”€ common/             # MÃ³dulos com funÃ§Ãµes utilitÃ¡rias reutilizÃ¡veis (ex: baixar pÃ¡ginas).
â”‚   â”œâ”€â”€ config/             # MÃ³dulos de configuraÃ§Ã£o, sem lÃ³gica de negÃ³cio (ex: paths, URLs, dicionÃ¡rios).
â”‚   â”œâ”€â”€ extract/            # MÃ³dulos responsÃ¡veis pela extraÃ§Ã£o (scraping) dos dados de cada fonte.
â”‚   â”œâ”€â”€ transform/          # MÃ³dulos responsÃ¡veis pela transformaÃ§Ã£o e agregaÃ§Ã£o (lÃ³gica das camadas Silver e Gold).
â”‚   â””â”€â”€ validation/         # MÃ³dulos para validaÃ§Ã£o de dados e esquemas (ex: schemas Pandera).
â”‚
â”œâ”€â”€ .gitignore              # Arquivo que define quais arquivos/pastas nÃ£o devem ser enviados ao Github.
â”œâ”€â”€ pipeline.py             # Script principal (entrypoint) que orquestra a execuÃ§Ã£o de todo o pipeline.
â”œâ”€â”€ requirements.txt        # Lista das dependÃªncias do projeto para fÃ¡cil instalaÃ§Ã£o.
â””â”€â”€ README.md               # DocumentaÃ§Ã£o do projeto (este arquivo).

## ğŸ¤” HipÃ³teses e DecisÃµes de Projeto
* **ResiliÃªncia a Falhas de SSL:** Foi implementado um fallback para tentativas de conexÃ£o sem verificaÃ§Ã£o de certificado, pois foi detectado um problema de SSL em uma das fontes.
* **ManutenÃ§Ã£o para Schema Drift:** O pipeline foi adaptado para lidar com uma mudanÃ§a no layout do site do Porto de Santos (remoÃ§Ã£o de uma coluna). A lÃ³gica de extraÃ§Ã£o agora Ã© mais robusta para lidar com colunas vazias.
* **ValidaÃ§Ã£o de Dados:** Foi utilizada a biblioteca `Pandera` para criar um script de validaÃ§Ã£o (`validate_silver.py`) que garante a integridade estrutural dos dados consolidados antes do processamento final, atuando como um portÃ£o de qualidade.
* **ConsolidaÃ§Ã£o de Colunas:** Foi desenvolvida uma funÃ§Ã£o auxiliar para consolidar colunas que, apÃ³s a renomeaÃ§Ã£o, ficavam duplicadas (ex: `sentido`, `produto`), garantindo um esquema limpo para a camada final.

## ğŸ”® PrÃ³ximos Passos e Melhorias
* **OrquestraÃ§Ã£o:** Integrar o pipeline com um orquestrador de workflows como Airflow ou Prefect para agendamento, monitoramento e retentativas automÃ¡ticas.
* **ContainerizaÃ§Ã£o:** Empacotar a aplicaÃ§Ã£o com Docker para garantir a portabilidade e facilitar o deploy em diferentes ambientes.
* **Testes:** Expandir a suÃ­te de testes unitÃ¡rios para as funÃ§Ãµes de transformaÃ§Ã£o e validaÃ§Ã£o, garantindo a qualidade do cÃ³digo.
* **Logging:** Substituir os comandos `print` por um sistema de logging mais robusto (ex: biblioteca `logging` do Python) para melhor controle e monitoramento dos eventos do pipeline.